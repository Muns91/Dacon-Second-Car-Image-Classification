{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "print(albumentations.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE': 384,\n",
    "    'BATCH_SIZE': 8,\n",
    "    'EPOCHS': 30,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            for fname in sorted(os.listdir(root_dir)):\n",
    "                if fname.lower().endswith('.jpg'):\n",
    "                    img_path = os.path.join(root_dir, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(root_dir, cls_name)\n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith('.jpg'):\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# 고정된 random seed로 항상 같은 샘플링 결과가 나오도록 설정\n",
    "random.seed(42)\n",
    "\n",
    "def get_classwise_indices(dataset, n_val_per_class):\n",
    "    class_to_indices = defaultdict(list)\n",
    "    \n",
    "    # dataset.samples에서 클래스별로 인덱스 수집\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        class_to_indices[label].append(idx)\n",
    "    \n",
    "    train_idx, val_idx = [], []\n",
    "\n",
    "    for label, indices in class_to_indices.items():\n",
    "        # 섞고 n개를 validation, 나머지는 train\n",
    "        random.shuffle(indices)\n",
    "        val_samples = indices[:n_val_per_class]\n",
    "        train_samples = indices[n_val_per_class:]\n",
    "\n",
    "        val_idx.extend(val_samples)\n",
    "        train_idx.extend(train_samples)\n",
    "\n",
    "    return train_idx, val_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    ''' CutMix augmentation '''\n",
    "    indices = torch.randperm(x.size(0))\n",
    "    shuffled_x = x[indices]\n",
    "    shuffled_y = y[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = shuffled_x[:, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    y_a, y_b = y, shuffled_y\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    ''' Random bounding box for CutMix '''\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def visualize_transform_effect_from_subset(subset_dataset, index=0):\n",
    "    image, label = subset_dataset[index]\n",
    "    image_aug = image.permute(1, 2, 0).cpu().numpy()\n",
    "    image_aug = (image_aug * 255).astype('uint8')\n",
    "\n",
    "    img_path, _ = subset_dataset.dataset.samples[subset_dataset.indices[index]]\n",
    "    print(f\"Trying to read: {img_path}\")\n",
    "\n",
    "    try:\n",
    "        image_orig = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Failed to load image using PIL: {img_path}, Error: {e}\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(image_orig)\n",
    "    axs[0].set_title(\"Original\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(image_aug)\n",
    "    axs[1].set_title(\"Transformed\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = './train'\n",
    "test_root = './test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "        A.CoarseDropout(\n",
    "            max_holes=2,\n",
    "            max_height=int(0.3 * 512),\n",
    "            max_width=int(0.3 * 512),\n",
    "            min_holes=1,\n",
    "            min_height=int(0.1 * 512),\n",
    "            min_width=int(0.1 * 512),\n",
    "            fill_value=(255, 255, 255),\n",
    "            p=0.5\n",
    "        ),\n",
    "        A.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1,\n",
    "            p=0.5\n",
    "        ),\n",
    "        #A.Lambda(image=lambda x: x[..., np.random.permutation(3)] if random.random() < 0.5 else x),  # RGB shift\n",
    "        A.Rotate(limit=180, p=0.1),\n",
    "        A.RandomResizedCrop(\n",
    "    height=512,  # 제거하거나\n",
    "    width=512,   # 제거하고\n",
    "    size=(512, 512),  # 추가\n",
    "    scale=(0.5, 1.0),\n",
    "    ratio=(0.75, 1.33),\n",
    "    p=0.25\n",
    "),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 로드\n",
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"총 이미지 수: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "# Subset + transform 각각 적용\n",
    "train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train 이미지 수: {len(train_dataset)}, valid 이미지 수: {len(val_dataset)}')\n",
    "\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    visualize_transform_effect_from_subset(train_dataset, index=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name='convnext_base', num_classes=10):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        self.head = nn.Linear(self.backbone.num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# from internimage import intern_image\n",
    "\n",
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self, model_name='internimage_b', num_classes=10):\n",
    "#         super(BaseModel, self).__init__()\n",
    "#         self.backbone = intern_image(pretrained=True, checkpoint_path=None, variant=model_name)\n",
    "#         # InternImage는 기본적으로 1024차원 출력입니다 (variant에 따라 다름)\n",
    "#         self.head = nn.Linear(self.backbone.embed_dim, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)\n",
    "#         x = self.head(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# full_dataset.samples -> [(img_path, label), ...]\n",
    "labels = [label for _, label in full_dataset.samples]\n",
    "train_labels = np.array(labels)[train_idx]  # train_idx에서 학습 데이터만 뽑음\n",
    "\n",
    "# class_weight 계산\n",
    "classes = np.unique(train_labels)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=train_labels)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"클래스 가중치: {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# 모델 정의 및 초기화\n",
    "model = BaseModel(num_classes=len(class_names)).to(device)\n",
    "best_logloss = float('inf')\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=CFG['LEARNING_RATE'], weight_decay=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
    "\n",
    "# 로그 저장용 리스트\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_loglosses = []\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    # ------------------------- TRAIN -------------------------\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if np.random.rand() < CFG.get('CUTMIX_PROB', 0.5):\n",
    "            images, targets1, targets2, lam = cutmix_data(images, labels)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets1) * lam + criterion(outputs, targets2) * (1. - lam)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "    # ------------------------- VALID -------------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # LogLoss\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "\n",
    "    # 로그 저장\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_loglosses.append(val_logloss)\n",
    "\n",
    "    # 출력\n",
    "    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | \"\n",
    "          f\"Valid Accuracy : {val_accuracy:.2f}% | LogLoss: {val_logloss:.4f}\")\n",
    "\n",
    "    # 🔹 스케줄러 업데이트\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"📉 Learning Rate after Epoch {epoch+1}: {current_lr:.6f}\")\n",
    "\n",
    "    # Best model 저장\n",
    "    if val_logloss < best_logloss:\n",
    "        best_logloss = val_logloss\n",
    "        torch.save(model.state_dict(), f'best_model.pth')\n",
    "        print(f\"📦 Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "\n",
    "# ------------------------- 그래프 시각화 -------------------------\n",
    "epochs = range(1, CFG['EPOCHS'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 1. Loss 그래프\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. Accuracy 그래프\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy (%)', color='green', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Validation Accuracy per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 3. LogLoss 그래프\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, val_loglosses, label='Validation LogLoss', color='red', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('LogLoss')\n",
    "plt.title('Validation LogLoss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import koreanize_matplotlib\n",
    "\n",
    "def imshow(img_tensor):\n",
    "    \"\"\" De-normalize 후 matplotlib로 출력 가능하게 변환 \"\"\"\n",
    "    img = img_tensor.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "\n",
    "def analyze_and_save_wrong_predictions(model, val_dataset, class_names, save_path='val_wrong_predictions.csv', device='cuda'):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = len(val_dataset)\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    wrong_records = []\n",
    "\n",
    "    for i in range(total):\n",
    "        image, label = val_dataset[i]\n",
    "        global_idx = val_dataset.indices[i] if isinstance(val_dataset, torch.utils.data.Subset) else i\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_input)\n",
    "            predicted = outputs.argmax(1).item()\n",
    "\n",
    "        if predicted == label:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "\n",
    "            # 원본 경로 얻기\n",
    "            if hasattr(val_dataset.dataset, 'samples'):\n",
    "                img_path = val_dataset.dataset.samples[global_idx][0]\n",
    "            else:\n",
    "                img_path = 'Unknown'\n",
    "\n",
    "            wrong_records.append({\n",
    "                'ID': i,\n",
    "                '데이터 경로': img_path,\n",
    "                '원래 라벨 이름': class_names[label],\n",
    "                '예측 라벨 이름': class_names[predicted]\n",
    "            })\n",
    "\n",
    "    # 통계 출력\n",
    "    print(f\"✅ 총 Validation 샘플 수: {total}\")\n",
    "    print(f\"🎯 정확하게 예측한 수: {correct}\")\n",
    "    print(f\"❌ 잘못 예측한 수: {wrong}\")\n",
    "\n",
    "    # 잘못 예측한 것만 CSV로 저장\n",
    "    df_wrong = pd.DataFrame(wrong_records)\n",
    "    #df_wrong.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"📁 잘못 예측한 결과 출력 완료!.\")\n",
    "\n",
    "    return df_wrong\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_wrong_predictions(model, val_dataset, class_names, device='cuda', start=0, n=8):\n",
    "    \"\"\"\n",
    "    예측이 틀린 Validation 이미지들만 순차적으로 시각화\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    wrong_samples = []\n",
    "\n",
    "    for idx in range(len(val_dataset)):\n",
    "        image, label = val_dataset[idx]\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_input)\n",
    "            predicted = outputs.argmax(1).item()\n",
    "\n",
    "        if predicted != label:\n",
    "            wrong_samples.append((image, label, predicted))\n",
    "\n",
    "    total_wrong = len(wrong_samples)\n",
    "    print(f\"\\n🖼️ 총 잘못 분류된 샘플 수: {total_wrong}\")\n",
    "\n",
    "    end = min(start + n, total_wrong)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    for i in range(start, end):\n",
    "        image, label, predicted = wrong_samples[i]\n",
    "        true_label_name = class_names[label]\n",
    "        pred_label_name = class_names[predicted]\n",
    "\n",
    "        plt.subplot(2, n // 2, i - start + 1)\n",
    "        plt.imshow(imshow(image.cpu()))\n",
    "        plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\", fontsize=12)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 분석 및 CSV 저장\n",
    "df = analyze_and_save_wrong_predictions(model, val_dataset, class_names, save_path='val_predictions.csv', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘못 예측한 행만 필터링\n",
    "df_wrong_only = df[df['원래 라벨 이름'] != df['예측 라벨 이름']].reset_index(drop=True)\n",
    "\n",
    "# ID 컬럼 제거\n",
    "df_wrong_only = df_wrong_only.drop(columns=['ID'])\n",
    "\n",
    "# CSV로 저장\n",
    "df_wrong_only.to_csv('val_wrong_only_v8.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"저장 완료: {len(df_wrong_only)}개의 잘못 예측된 샘플을 'val_wrong_only_2.csv'에 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 틀린 예측 시각화 (9번부터 16개 보기)\n",
    "visualize_wrong_predictions(model, val_dataset, class_names, device=device, start=9, n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_gradcam(model, image_tensor, target_class, target_layer, device='cuda'):\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    activations = {}\n",
    "    gradients = {}\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations['value'] = output\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients['value'] = grad_output[0]\n",
    "\n",
    "    # Register hooks\n",
    "    handle_fw = target_layer.register_forward_hook(forward_hook)\n",
    "    handle_bw = target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    output = model(image_tensor)\n",
    "    model.zero_grad()\n",
    "    one_hot = torch.zeros_like(output)\n",
    "    one_hot[0][target_class] = 1\n",
    "    output.backward(gradient=one_hot)\n",
    "\n",
    "    # Get stored activations and gradients\n",
    "    act = activations['value'].squeeze(0)  # [C, H, W]\n",
    "    grad = gradients['value'].squeeze(0)   # [C, H, W]\n",
    "\n",
    "    weights = grad.mean(dim=(1, 2))  # GAP\n",
    "    cam = torch.sum(weights[:, None, None] * act, dim=0)\n",
    "\n",
    "    cam = cam.cpu().detach().numpy()\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / cam.max()\n",
    "    cam = cv2.resize(cam, (image_tensor.size(3), image_tensor.size(2)))\n",
    "\n",
    "    # Cleanup\n",
    "    handle_fw.remove()\n",
    "    handle_bw.remove()\n",
    "\n",
    "    return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gradcam_on_image(image_tensor, cam):\n",
    "    img = imshow(image_tensor.cpu())\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam_img = heatmap + img\n",
    "    cam_img = cam_img / np.max(cam_img)\n",
    "    return cam_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def visualize_wrong_with_gradcam(model, val_dataset, class_names, device='cuda', start=0, n=8, save_dir='wrong_predictions_gradcam'):\n",
    "    model.eval()\n",
    "    wrong_samples = []\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)  # 저장할 폴더 생성\n",
    "\n",
    "    # 잘못 예측한 샘플 수집\n",
    "    for idx in range(len(val_dataset)):\n",
    "        image, label = val_dataset[idx]\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_input)\n",
    "            predicted = outputs.argmax(1).item()\n",
    "\n",
    "        if predicted != label:\n",
    "            wrong_samples.append((image, label, predicted, idx))  # 인덱스 포함\n",
    "\n",
    "    # Grad-CAM 시각화 저장 (전체)\n",
    "    for i, (image, label, predicted, data_idx) in enumerate(wrong_samples):\n",
    "        try:\n",
    "            if hasattr(model, 'backbone') and hasattr(model.backbone, 'stages'):\n",
    "                target_layer = list(model.backbone.stages[-1].children())[-1]\n",
    "            else:\n",
    "                target_layer = list(model.children())[-1]\n",
    "        except Exception as e:\n",
    "            print(\"Grad-CAM target layer 설정 실패:\", e)\n",
    "            return\n",
    "\n",
    "        # Grad-CAM 생성\n",
    "        cam = generate_gradcam(model, image, predicted, target_layer, device)\n",
    "        gradcam_img = show_gradcam_on_image(image, cam)\n",
    "\n",
    "        # 이미지 저장\n",
    "        true_label_name = class_names[label]\n",
    "        pred_label_name = class_names[predicted]\n",
    "        save_path = os.path.join(\n",
    "            save_dir,\n",
    "            f\"wrong_{i:03d}_idx_{data_idx}_true_{true_label_name}_pred_{pred_label_name}.png\"\n",
    "        )\n",
    "        gradcam_img_pil = to_pil_image(gradcam_img)\n",
    "        gradcam_img_pil.save(save_path)\n",
    "\n",
    "    # 일부 시각화\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    end = min(start + n, len(wrong_samples))\n",
    "\n",
    "    for i in range(start, end):\n",
    "        image, label, predicted, _ = wrong_samples[i]\n",
    "        cam = generate_gradcam(model, image, predicted, target_layer, device)\n",
    "        gradcam_img = show_gradcam_on_image(image, cam)\n",
    "\n",
    "        true_label_name = class_names[label]\n",
    "        pred_label_name = class_names[predicted]\n",
    "\n",
    "        plt.subplot(2, n // 2, i - start + 1)\n",
    "        plt.imshow(gradcam_img)\n",
    "        plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\", fontsize=12)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_wrong_with_gradcam(model, val_dataset, class_names, device=device, start=9, n=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset(test_root, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 로드\n",
    "model = BaseModel(num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # 각 배치의 확률을 리스트로 변환\n",
    "        for prob in probs.cpu():  # prob: (num_classes,)\n",
    "            result = {\n",
    "                class_names[i]: prob[i].item()\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "pred = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 'ID' 컬럼을 제외한 클래스 컬럼 정렬\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('submit41_size_384.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
