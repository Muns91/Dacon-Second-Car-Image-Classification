{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "print(albumentations.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE': 384,\n",
    "    'BATCH_SIZE': 8,\n",
    "    'EPOCHS': 30,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed Í≥†Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            for fname in sorted(os.listdir(root_dir)):\n",
    "                if fname.lower().endswith('.jpg'):\n",
    "                    img_path = os.path.join(root_dir, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(root_dir, cls_name)\n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith('.jpg'):\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Í≥†Ï†ïÎêú random seedÎ°ú Ìï≠ÏÉÅ Í∞ôÏùÄ ÏÉòÌîåÎßÅ Í≤∞Í≥ºÍ∞Ä ÎÇòÏò§ÎèÑÎ°ù ÏÑ§Ï†ï\n",
    "random.seed(42)\n",
    "\n",
    "def get_classwise_indices(dataset, n_val_per_class):\n",
    "    class_to_indices = defaultdict(list)\n",
    "    \n",
    "    # dataset.samplesÏóêÏÑú ÌÅ¥ÎûòÏä§Î≥ÑÎ°ú Ïù∏Îç±Ïä§ ÏàòÏßë\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        class_to_indices[label].append(idx)\n",
    "    \n",
    "    train_idx, val_idx = [], []\n",
    "\n",
    "    for label, indices in class_to_indices.items():\n",
    "        # ÏÑûÍ≥† nÍ∞úÎ•º validation, ÎÇòÎ®∏ÏßÄÎäî train\n",
    "        random.shuffle(indices)\n",
    "        val_samples = indices[:n_val_per_class]\n",
    "        train_samples = indices[n_val_per_class:]\n",
    "\n",
    "        val_idx.extend(val_samples)\n",
    "        train_idx.extend(train_samples)\n",
    "\n",
    "    return train_idx, val_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    ''' CutMix augmentation '''\n",
    "    indices = torch.randperm(x.size(0))\n",
    "    shuffled_x = x[indices]\n",
    "    shuffled_y = y[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = shuffled_x[:, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    y_a, y_b = y, shuffled_y\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    ''' Random bounding box for CutMix '''\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def visualize_transform_effect_from_subset(subset_dataset, index=0):\n",
    "    image, label = subset_dataset[index]\n",
    "    image_aug = image.permute(1, 2, 0).cpu().numpy()\n",
    "    image_aug = (image_aug * 255).astype('uint8')\n",
    "\n",
    "    img_path, _ = subset_dataset.dataset.samples[subset_dataset.indices[index]]\n",
    "    print(f\"Trying to read: {img_path}\")\n",
    "\n",
    "    try:\n",
    "        image_orig = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Failed to load image using PIL: {img_path}, Error: {e}\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(image_orig)\n",
    "    axs[0].set_title(\"Original\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(image_aug)\n",
    "    axs[1].set_title(\"Transformed\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = './train'\n",
    "test_root = './test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "        A.CoarseDropout(\n",
    "            max_holes=2,\n",
    "            max_height=int(0.3 * 512),\n",
    "            max_width=int(0.3 * 512),\n",
    "            min_holes=1,\n",
    "            min_height=int(0.1 * 512),\n",
    "            min_width=int(0.1 * 512),\n",
    "            fill_value=(255, 255, 255),\n",
    "            p=0.5\n",
    "        ),\n",
    "        A.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1,\n",
    "            p=0.5\n",
    "        ),\n",
    "        #A.Lambda(image=lambda x: x[..., np.random.permutation(3)] if random.random() < 0.5 else x),  # RGB shift\n",
    "        A.Rotate(limit=180, p=0.1),\n",
    "        A.RandomResizedCrop(\n",
    "    height=512,  # Ï†úÍ±∞ÌïòÍ±∞ÎÇò\n",
    "    width=512,   # Ï†úÍ±∞ÌïòÍ≥†\n",
    "    size=(512, 512),  # Ï∂îÍ∞Ä\n",
    "    scale=(0.5, 1.0),\n",
    "    ratio=(0.75, 1.33),\n",
    "    p=0.25\n",
    "),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"Ï¥ù Ïù¥ÎØ∏ÏßÄ Ïàò: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "# Subset + transform Í∞ÅÍ∞Å Ï†ÅÏö©\n",
    "train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train Ïù¥ÎØ∏ÏßÄ Ïàò: {len(train_dataset)}, valid Ïù¥ÎØ∏ÏßÄ Ïàò: {len(val_dataset)}')\n",
    "\n",
    "\n",
    "# DataLoader Ï†ïÏùò\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    visualize_transform_effect_from_subset(train_dataset, index=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name='convnext_base', num_classes=10):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        self.head = nn.Linear(self.backbone.num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# from internimage import intern_image\n",
    "\n",
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self, model_name='internimage_b', num_classes=10):\n",
    "#         super(BaseModel, self).__init__()\n",
    "#         self.backbone = intern_image(pretrained=True, checkpoint_path=None, variant=model_name)\n",
    "#         # InternImageÎäî Í∏∞Î≥∏Ï†ÅÏúºÎ°ú 1024Ï∞®Ïõê Ï∂úÎ†•ÏûÖÎãàÎã§ (variantÏóê Îî∞Îùº Îã§Î¶Ñ)\n",
    "#         self.head = nn.Linear(self.backbone.embed_dim, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)\n",
    "#         x = self.head(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# full_dataset.samples -> [(img_path, label), ...]\n",
    "labels = [label for _, label in full_dataset.samples]\n",
    "train_labels = np.array(labels)[train_idx]  # train_idxÏóêÏÑú ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Îßå ÎΩëÏùå\n",
    "\n",
    "# class_weight Í≥ÑÏÇ∞\n",
    "classes = np.unique(train_labels)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=train_labels)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò: {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Î™®Îç∏ Ï†ïÏùò Î∞è Ï¥àÍ∏∞Ìôî\n",
    "model = BaseModel(num_classes=len(class_names)).to(device)\n",
    "best_logloss = float('inf')\n",
    "\n",
    "# ÏÜêÏã§ Ìï®Ïàò Î∞è ÏòµÌã∞ÎßàÏù¥Ï†Ä\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=CFG['LEARNING_RATE'], weight_decay=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
    "\n",
    "# Î°úÍ∑∏ Ï†ÄÏû•Ïö© Î¶¨Ïä§Ìä∏\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_loglosses = []\n",
    "\n",
    "# ÌïôÏäµ Î£®ÌîÑ\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    # ------------------------- TRAIN -------------------------\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if np.random.rand() < CFG.get('CUTMIX_PROB', 0.5):\n",
    "            images, targets1, targets2, lam = cutmix_data(images, labels)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets1) * lam + criterion(outputs, targets2) * (1. - lam)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "    # ------------------------- VALID -------------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # LogLoss\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "\n",
    "    # Î°úÍ∑∏ Ï†ÄÏû•\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_loglosses.append(val_logloss)\n",
    "\n",
    "    # Ï∂úÎ†•\n",
    "    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | \"\n",
    "          f\"Valid Accuracy : {val_accuracy:.2f}% | LogLoss: {val_logloss:.4f}\")\n",
    "\n",
    "    # üîπ Ïä§ÏºÄÏ§ÑÎü¨ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"üìâ Learning Rate after Epoch {epoch+1}: {current_lr:.6f}\")\n",
    "\n",
    "    # Best model Ï†ÄÏû•\n",
    "    if val_logloss < best_logloss:\n",
    "        best_logloss = val_logloss\n",
    "        torch.save(model.state_dict(), f'best_model.pth')\n",
    "        print(f\"üì¶ Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "\n",
    "# ------------------------- Í∑∏ÎûòÌîÑ ÏãúÍ∞ÅÌôî -------------------------\n",
    "epochs = range(1, CFG['EPOCHS'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 1. Loss Í∑∏ÎûòÌîÑ\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. Accuracy Í∑∏ÎûòÌîÑ\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy (%)', color='green', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Validation Accuracy per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 3. LogLoss Í∑∏ÎûòÌîÑ\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, val_loglosses, label='Validation LogLoss', color='red', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('LogLoss')\n",
    "plt.title('Validation LogLoss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import koreanize_matplotlib\n",
    "\n",
    "def imshow(img_tensor):\n",
    "    \"\"\" De-normalize ÌõÑ matplotlibÎ°ú Ï∂úÎ†• Í∞ÄÎä•ÌïòÍ≤å Î≥ÄÌôò \"\"\"\n",
    "    img = img_tensor.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "\n",
    "def analyze_and_save_wrong_predictions(model, val_dataset, class_names, save_path='val_wrong_predictions.csv', device='cuda'):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = len(val_dataset)\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    wrong_records = []\n",
    "\n",
    "    for i in range(total):\n",
    "        image, label = val_dataset[i]\n",
    "        global_idx = val_dataset.indices[i] if isinstance(val_dataset, torch.utils.data.Subset) else i\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_input)\n",
    "            predicted = outputs.argmax(1).item()\n",
    "\n",
    "        if predicted == label:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "\n",
    "            # ÏõêÎ≥∏ Í≤ΩÎ°ú ÏñªÍ∏∞\n",
    "            if hasattr(val_dataset.dataset, 'samples'):\n",
    "                img_path = val_dataset.dataset.samples[global_idx][0]\n",
    "            else:\n",
    "                img_path = 'Unknown'\n",
    "\n",
    "            wrong_records.append({\n",
    "                'ID': i,\n",
    "                'Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú': img_path,\n",
    "                'ÏõêÎûò ÎùºÎ≤® Ïù¥Î¶Ñ': class_names[label],\n",
    "                'ÏòàÏ∏° ÎùºÎ≤® Ïù¥Î¶Ñ': class_names[predicted]\n",
    "            })\n",
    "\n",
    "    # ÌÜµÍ≥Ñ Ï∂úÎ†•\n",
    "    print(f\"‚úÖ Ï¥ù Validation ÏÉòÌîå Ïàò: {total}\")\n",
    "    print(f\"üéØ Ï†ïÌôïÌïòÍ≤å ÏòàÏ∏°Ìïú Ïàò: {correct}\")\n",
    "    print(f\"‚ùå ÏûòÎ™ª ÏòàÏ∏°Ìïú Ïàò: {wrong}\")\n",
    "\n",
    "    # ÏûòÎ™ª ÏòàÏ∏°Ìïú Í≤ÉÎßå CSVÎ°ú Ï†ÄÏû•\n",
    "    df_wrong = pd.DataFrame(wrong_records)\n",
    "    #df_wrong.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"üìÅ ÏûòÎ™ª ÏòàÏ∏°Ìïú Í≤∞Í≥º Ï∂úÎ†• ÏôÑÎ£å!.\")\n",
    "\n",
    "    return df_wrong\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_wrong_predictions(model, val_dataset, class_names, device='cuda', start=0, n=8):\n",
    "    \"\"\"\n",
    "    ÏòàÏ∏°Ïù¥ ÌãÄÎ¶∞ Validation Ïù¥ÎØ∏ÏßÄÎì§Îßå ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÏãúÍ∞ÅÌôî\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    wrong_samples = []\n",
    "\n",
    "    for idx in range(len(val_dataset)):\n",
    "        image, label = val_dataset[idx]\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_input)\n",
    "            predicted = outputs.argmax(1).item()\n",
    "\n",
    "        if predicted != label:\n",
    "            wrong_samples.append((image, label, predicted))\n",
    "\n",
    "    total_wrong = len(wrong_samples)\n",
    "    print(f\"\\nüñºÔ∏è Ï¥ù ÏûòÎ™ª Î∂ÑÎ•òÎêú ÏÉòÌîå Ïàò: {total_wrong}\")\n",
    "\n",
    "    end = min(start + n, total_wrong)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    for i in range(start, end):\n",
    "        image, label, predicted = wrong_samples[i]\n",
    "        true_label_name = class_names[label]\n",
    "        pred_label_name = class_names[predicted]\n",
    "\n",
    "        plt.subplot(2, n // 2, i - start + 1)\n",
    "        plt.imshow(imshow(image.cpu()))\n",
    "        plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\", fontsize=12)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏòàÏ∏° Î∂ÑÏÑù Î∞è CSV Ï†ÄÏû•\n",
    "df = analyze_and_save_wrong_predictions(model, val_dataset, class_names, save_path='val_predictions.csv', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏûòÎ™ª ÏòàÏ∏°Ìïú ÌñâÎßå ÌïÑÌÑ∞ÎßÅ\n",
    "df_wrong_only = df[df['ÏõêÎûò ÎùºÎ≤® Ïù¥Î¶Ñ'] != df['ÏòàÏ∏° ÎùºÎ≤® Ïù¥Î¶Ñ']].reset_index(drop=True)\n",
    "\n",
    "# ID Ïª¨Îüº Ï†úÍ±∞\n",
    "df_wrong_only = df_wrong_only.drop(columns=['ID'])\n",
    "\n",
    "# CSVÎ°ú Ï†ÄÏû•\n",
    "df_wrong_only.to_csv('val_wrong_only_v8.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Í≤∞Í≥º ÌôïÏù∏\n",
    "print(f\"Ï†ÄÏû• ÏôÑÎ£å: {len(df_wrong_only)}Í∞úÏùò ÏûòÎ™ª ÏòàÏ∏°Îêú ÏÉòÌîåÏùÑ 'val_wrong_only_2.csv'Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌãÄÎ¶∞ ÏòàÏ∏° ÏãúÍ∞ÅÌôî (9Î≤àÎ∂ÄÌÑ∞ 16Í∞ú Î≥¥Í∏∞)\n",
    "visualize_wrong_predictions(model, val_dataset, class_names, device=device, start=9, n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_gradcam(model, image_tensor, target_class, target_layer, device='cuda'):\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    activations = {}\n",
    "    gradients = {}\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations['value'] = output\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients['value'] = grad_output[0]\n",
    "\n",
    "    # Register hooks\n",
    "    handle_fw = target_layer.register_forward_hook(forward_hook)\n",
    "    handle_bw = target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    output = model(image_tensor)\n",
    "    model.zero_grad()\n",
    "    one_hot = torch.zeros_like(output)\n",
    "    one_hot[0][target_class] = 1\n",
    "    output.backward(gradient=one_hot)\n",
    "\n",
    "    # Get stored activations and gradients\n",
    "    act = activations['value'].squeeze(0)  # [C, H, W]\n",
    "    grad = gradients['value'].squeeze(0)   # [C, H, W]\n",
    "\n",
    "    weights = grad.mean(dim=(1, 2))  # GAP\n",
    "    cam = torch.sum(weights[:, None, None] * act, dim=0)\n",
    "\n",
    "    cam = cam.cpu().detach().numpy()\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / cam.max()\n",
    "    cam = cv2.resize(cam, (image_tensor.size(3), image_tensor.size(2)))\n",
    "\n",
    "    # Cleanup\n",
    "    handle_fw.remove()\n",
    "    handle_bw.remove()\n",
    "\n",
    "    return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gradcam_on_image(image_tensor, cam):\n",
    "    img = imshow(image_tensor.cpu())\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam_img = heatmap + img\n",
    "    cam_img = cam_img / np.max(cam_img)\n",
    "    return cam_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def visualize_wrong_with_gradcam(model, val_dataset, class_names, device='cuda', start=0, n=8, save_dir='wrong_predictions_gradcam'):\n",
    "    model.eval()\n",
    "    wrong_samples = []\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ï†ÄÏû•Ìï† Ìè¥Îçî ÏÉùÏÑ±\n",
    "\n",
    "    # ÏûòÎ™ª ÏòàÏ∏°Ìïú ÏÉòÌîå ÏàòÏßë\n",
    "    for idx in range(len(val_dataset)):\n",
    "        image, label = val_dataset[idx]\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_input)\n",
    "            predicted = outputs.argmax(1).item()\n",
    "\n",
    "        if predicted != label:\n",
    "            wrong_samples.append((image, label, predicted, idx))  # Ïù∏Îç±Ïä§ Ìè¨Ìï®\n",
    "\n",
    "    # Grad-CAM ÏãúÍ∞ÅÌôî Ï†ÄÏû• (Ï†ÑÏ≤¥)\n",
    "    for i, (image, label, predicted, data_idx) in enumerate(wrong_samples):\n",
    "        try:\n",
    "            if hasattr(model, 'backbone') and hasattr(model.backbone, 'stages'):\n",
    "                target_layer = list(model.backbone.stages[-1].children())[-1]\n",
    "            else:\n",
    "                target_layer = list(model.children())[-1]\n",
    "        except Exception as e:\n",
    "            print(\"Grad-CAM target layer ÏÑ§Ï†ï Ïã§Ìå®:\", e)\n",
    "            return\n",
    "\n",
    "        # Grad-CAM ÏÉùÏÑ±\n",
    "        cam = generate_gradcam(model, image, predicted, target_layer, device)\n",
    "        gradcam_img = show_gradcam_on_image(image, cam)\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•\n",
    "        true_label_name = class_names[label]\n",
    "        pred_label_name = class_names[predicted]\n",
    "        save_path = os.path.join(\n",
    "            save_dir,\n",
    "            f\"wrong_{i:03d}_idx_{data_idx}_true_{true_label_name}_pred_{pred_label_name}.png\"\n",
    "        )\n",
    "        gradcam_img_pil = to_pil_image(gradcam_img)\n",
    "        gradcam_img_pil.save(save_path)\n",
    "\n",
    "    # ÏùºÎ∂Ä ÏãúÍ∞ÅÌôî\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    end = min(start + n, len(wrong_samples))\n",
    "\n",
    "    for i in range(start, end):\n",
    "        image, label, predicted, _ = wrong_samples[i]\n",
    "        cam = generate_gradcam(model, image, predicted, target_layer, device)\n",
    "        gradcam_img = show_gradcam_on_image(image, cam)\n",
    "\n",
    "        true_label_name = class_names[label]\n",
    "        pred_label_name = class_names[predicted]\n",
    "\n",
    "        plt.subplot(2, n // 2, i - start + 1)\n",
    "        plt.imshow(gradcam_img)\n",
    "        plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\", fontsize=12)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_wrong_with_gradcam(model, val_dataset, class_names, device=device, start=9, n=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset(test_root, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ÄÏû•Îêú Î™®Îç∏ Î°úÎìú\n",
    "model = BaseModel(num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# Ï∂îÎ°†\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # Í∞Å Î∞∞ÏπòÏùò ÌôïÎ•†ÏùÑ Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôò\n",
    "        for prob in probs.cpu():  # prob: (num_classes,)\n",
    "            result = {\n",
    "                class_names[i]: prob[i].item()\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "pred = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 'ID' Ïª¨ÎüºÏùÑ Ï†úÏô∏Ìïú ÌÅ¥ÎûòÏä§ Ïª¨Îüº Ï†ïÎ†¨\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('submit41_size_384.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
